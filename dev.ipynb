{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-07-14T16:57:52.452-0600\u001b[0m] {\u001b[34mutils.py:\u001b[0m148} INFO\u001b[0m - Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[\u001b[34m2024-07-14T16:57:52.461-0600\u001b[0m] {\u001b[34mutils.py:\u001b[0m160} INFO\u001b[0m - NumExpr defaulting to 8 threads.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "# import matplotlib.pyplot as plt\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType, DateType\n",
    "from pyspark.sql.functions import isnan, isnull, when, count, col, to_date, udf\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "# from airflow.hooks.postgres_hook import PostgresHook\n",
    "# from airflow.hooks.S3_hook import S3Hook\n",
    "# import pandas as pd\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "from constants import COVID_EPIDEMIOLOGY_FILE, COVID_HOSPITALIZATIONS_FILE, SP500_FILE, NASDAQ_FILE, COVID_ECONOMY_FILE, COVID_WEATHER_FILE, \\\n",
    "    RDS_ENDPOINT, RDS_PORT, RDS_USERNAME, RDS_PASSWORD, RDS_DB_NAME, JDBC_DRIVER\n",
    "from aws_helpers import read_s3_to_spark, spark_write_to_rds, spark_read_from_rds\n",
    "from spark_helpers import get_spark_session_and_context\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "# airflow standalone\n",
    "# give ec2 access to s3 using iam roles\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/07/14 16:57:54 WARN Utils: Your hostname, PeterNguyen resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/07/14 16:57:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/07/14 16:57:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark, sc = get_spark_session_and_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-22\n",
      "2021-03-19\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(  # engine\n",
    "    database=RDS_DB_NAME,\n",
    "    user=RDS_USERNAME,\n",
    "    password=RDS_PASSWORD,\n",
    "    host=RDS_ENDPOINT,\n",
    "    port=RDS_PORT\n",
    ")\n",
    "\n",
    "curr = conn.cursor()\n",
    "curr.execute(\"SELECT date FROM sp_five_hundred LIMIT(2);\")\n",
    "\n",
    "rows = curr.fetchall()\n",
    "for row in rows:\n",
    "    print(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-07-14T16:58:13.737-0600\u001b[0m] {\u001b[34mcredentials.py:\u001b[0m621} INFO\u001b[0m - Found credentials in shared credentials file: ~/.aws/credentials\u001b[0m\n",
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close/Last: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+-------+-------+\n",
      "|      Date|Close/Last|   Open|   High|    Low|\n",
      "+----------+----------+-------+-------+-------+\n",
      "|07/12/2024|   5615.35|5590.76|5655.56|5590.44|\n",
      "|07/11/2024|   5584.54|5635.21|5642.45|5576.53|\n",
      "|07/10/2024|   5633.91|5591.26|5635.39|5586.44|\n",
      "|07/09/2024|   5576.98|5584.24|5590.75|5574.57|\n",
      "|07/08/2024|   5572.85|5572.75|5583.11|5562.51|\n",
      "+----------+----------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sp500_df = read_s3_to_spark(SP500_FILE, spark)  # seemed simpler to read s3 into pandas then spark\n",
    "nasdaq_df = read_s3_to_spark(NASDAQ_FILE, spark)\n",
    "\n",
    "sp500_df.printSchema()\n",
    "sp500_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date format\n",
    "def format_stock_df(df):\n",
    "    df = df.withColumn('Date', to_date(col('Date'), 'MM/dd/yyyy')).withColumnRenamed('Date', 'date')  # specify the given date\n",
    "    df = df.withColumnRenamed('Close/Last', 'close')\n",
    "    df = df.withColumnRenamed('Open', 'open')\n",
    "    df = df.withColumnRenamed('High', 'high')\n",
    "    df = df.withColumnRenamed('Low', 'low')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sp500_df = format_stock_df(sp500_df)\n",
    "nasdaq_df = format_stock_df(nasdaq_df)\n",
    "\n",
    "spark_write_to_rds(sp500_df, table_name=\"sp_five_hundred\")\n",
    "spark_write_to_rds(nasdaq_df, table_name=\"nasdaq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"data/covid/epidemiology.json\"\n",
    "\n",
    "import json\n",
    "with open(data_file, \"r\") as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = spark.read.json(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\"conf\": {\"spark.jars.packages\": \"org.mongodb.spark:mongo-spark-connector_2.11:2.3.2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['columns', 'data'])\n",
      "<class 'list'>\n",
      "12525378\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())\n",
    "columns = data['columns']\n",
    "\n",
    "# schema = StructType()\n",
    "print(type(data['data']))\n",
    "print(len(data['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sp500_df\n",
    "\n",
    "print(\"null count\")\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# print(\"nan count\")  # none\n",
    "# df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Drop Nulls\n",
    "cleaned_df = df.na.drop()  # subset=['gdp_usd', 'gdp_per_capita_usd']\n",
    "# df.na.fill({\"age\": 0, \"name\": \"unknown\"}).show()  # Fill missing values\n",
    "\n",
    "\n",
    "for col_name in ['new_confirmed', 'new_deceased', 'new_recovered', 'new_tested']:\n",
    "    print(f\"Changing {cleaned_df.where(col(col_name) < 0).select(col(col_name)).count()}, negative values in {col_name} to 0\")\n",
    "\n",
    "    cleaned_df = cleaned_df.withColumn(col_name, when(col(col_name) < 0, 0).otherwise(col(col_name)))\n",
    "\n",
    "# print(\"Duplicates\")  # none\n",
    "duplicate_rows = cleaned_df.count() - cleaned_df.dropDuplicates().count()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "cleaned_df = cleaned_df.dropDuplicates()\n",
    "cleaned_df.limit(10).show()\n",
    "print(f\"Reduced {df.count()} rows to {cleaned_df.count()} rows\")\n",
    "claned_epidemiology_df = cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|      date|location_key|new_confirmed|new_deceased|new_recovered|new_tested|cumulative_confirmed|cumulative_deceased|cumulative_recovered|cumulative_tested|\n",
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|2020-01-01|          AD|            0|           0|         null|      null|                   0|                  0|                null|             null|\n",
      "|2020-01-02|          AD|            0|           0|         null|      null|                   0|                  0|                null|             null|\n",
      "|2020-01-03|          AD|            0|           0|         null|      null|                   0|                  0|                null|             null|\n",
      "|2020-01-04|          AD|            0|           0|         null|      null|                   0|                  0|                null|             null|\n",
      "|2020-01-05|          AD|            0|           0|         null|      null|                   0|                  0|                null|             null|\n",
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/GoogleCloudPlatform/covid-19-open-data/blob/main/docs/table-epidemiology.md\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('date', DateType(), True),\n",
    "    StructField('location_key', StringType(), True),\n",
    "    StructField('new_confirmed', IntegerType(), True),\n",
    "    StructField('new_deceased', IntegerType(), True),\n",
    "    StructField('new_recovered', IntegerType(), True),\n",
    "    StructField('new_tested', IntegerType(), True),\n",
    "    StructField('cumulative_confirmed', IntegerType(), True),\n",
    "    StructField('cumulative_deceased', IntegerType(), True),\n",
    "    StructField('cumulative_recovered', IntegerType(), True),\n",
    "    StructField('cumulative_tested', IntegerType(), True)\n",
    "])\n",
    "\n",
    "epidemiology_df = spark.read.format(\"csv\").schema(schema).option(\"header\", True).load(COVID_EPIDEMIOLOGY_FILE)\n",
    "\n",
    "epidemiology_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|date|location_key|new_confirmed|new_deceased|new_recovered|new_tested|cumulative_confirmed|cumulative_deceased|cumulative_recovered|cumulative_tested|\n",
      "+----+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|   0|           0|        50025|      858687|      8545363|   9331336|              198780|            1051000|             8534668|          9512906|\n",
      "+----+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing 26453, negative values in new_confirmed to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing 6986, negative values in new_deceased to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing 1888, negative values in new_recovered to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing 2769, negative values in new_tested to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|      date|location_key|new_confirmed|new_deceased|new_recovered|new_tested|cumulative_confirmed|cumulative_deceased|cumulative_recovered|cumulative_tested|\n",
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|2020-12-23|          AT|         2843|          99|         3115|     38440|              344938|               6994|              307537|          3682136|\n",
      "|2021-02-07|          AT|          993|          40|         1052|    212647|              420176|               9780|              391279|         10313170|\n",
      "|2021-06-08|          AT|          356|           4|          607|    412011|              641360|              12964|              620881|         45983005|\n",
      "|2021-08-15|          AT|          797|           1|          432|    395705|              663338|              13154|              638777|         71577449|\n",
      "|2021-10-02|          AT|         1555|          15|         1785|    225990|              742938|              13522|              702140|         87701064|\n",
      "|2022-05-26|          AT|         1736|           7|         2924|    259173|             4265951|              19893|             4219052|        187227485|\n",
      "|2021-04-25|        AT_1|           54|           0|           49|      6199|               17290|                384|               16213|          1254623|\n",
      "|2021-11-23|        AT_1|          340|           0|          347|      5140|               28100|                457|               23923|          3388215|\n",
      "|2021-11-01|        AT_2|          350|           1|          184|      5829|               49890|               1110|               45898|          4801002|\n",
      "|2022-04-09|        AT_2|          512|           2|         1155|      1547|              234624|               1588|              226771|          6489938|\n",
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 474:=============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced 12525825 rows to 2450433 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = epidemiology_df\n",
    "\n",
    "print(\"null count\")\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# print(\"nan count\")  # none\n",
    "# df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Drop Nulls\n",
    "cleaned_df = df.na.drop()  # subset=['gdp_usd', 'gdp_per_capita_usd']\n",
    "# df.na.fill({\"age\": 0, \"name\": \"unknown\"}).show()  # Fill missing values\n",
    "\n",
    "\n",
    "for col_name in ['new_confirmed', 'new_deceased', 'new_recovered', 'new_tested']:\n",
    "    print(f\"Changing {cleaned_df.where(col(col_name) < 0).select(col(col_name)).count()}, negative values in {col_name} to 0\")\n",
    "\n",
    "    cleaned_df = cleaned_df.withColumn(col_name, when(col(col_name) < 0, 0).otherwise(col(col_name)))\n",
    "\n",
    "# print(\"Duplicates\")  # none\n",
    "duplicate_rows = cleaned_df.count() - cleaned_df.dropDuplicates().count()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "cleaned_df = cleaned_df.dropDuplicates()\n",
    "cleaned_df.limit(10).show()\n",
    "print(f\"Reduced {df.count()} rows to {cleaned_df.count()} rows\")\n",
    "claned_epidemiology_df = cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+-------------------+\n",
      "|location_key|     gdp_usd|gdp_per_capita_usd|human_capital_index|\n",
      "+------------+------------+------------------+-------------------+\n",
      "|          AD|  3154057987|             40886|               null|\n",
      "|          AE|421142267937|             43103|              0.659|\n",
      "|          AF| 19101353832|               502|              0.389|\n",
      "|          AG|  1727759259|             17790|               null|\n",
      "|          AL| 15278077446|              5352|              0.621|\n",
      "+------------+------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/GoogleCloudPlatform/covid-19-open-data/blob/main/docs/table-economy.md\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('location_key', StringType(), True),\n",
    "    StructField('gdp_usd', LongType(), True),\n",
    "    StructField('gdp_per_capita_usd', IntegerType(), True),\n",
    "    StructField('human_capital_index', FloatType(), True)\n",
    "])\n",
    "\n",
    "economy_df = spark.read.format(\"csv\").schema(schema).option(\"header\", True).load(COVID_ECONOMY_FILE)\n",
    "\n",
    "economy_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null count\n",
      "+------------+-------+------------------+-------------------+\n",
      "|location_key|gdp_usd|gdp_per_capita_usd|human_capital_index|\n",
      "+------------+-------+------------------+-------------------+\n",
      "|           0|     31|                39|                248|\n",
      "+------------+-------+------------------+-------------------+\n",
      "\n",
      "TODO: still need to handle human_capital_index\n",
      "Reduced 404 rows to 334 rows\n",
      "Duplicates\n",
      "Number of duplicate rows: 0\n",
      "+------------+------------+------------------+-------------------+\n",
      "|location_key|     gdp_usd|gdp_per_capita_usd|human_capital_index|\n",
      "+------------+------------+------------------+-------------------+\n",
      "|       DE_ST| 73969539000|             33394|               null|\n",
      "|       IT_25|457916381400|             45548|               null|\n",
      "|        AT_5| 34272274000|             61832|               null|\n",
      "|       IT_88| 41212125400|             25016|               null|\n",
      "|          GN| 13590281808|              1064|              0.374|\n",
      "|      BE_BRU| 99104176200|             81892|               null|\n",
      "|      BE_WNA| 15884452000|             31978|               null|\n",
      "|          CL|282318159744|             14896|              0.674|\n",
      "|       NL_GR| 29455974200|             50504|               null|\n",
      "|       NL_ZE| 16022665400|             41890|               null|\n",
      "+------------+------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"null count\")\n",
    "economy_df.select([count(when(isnull(c), c)).alias(c) for c in economy_df.columns]).show()\n",
    "# print(\"nan count\")  # none\n",
    "# economy_df.select([count(when(isnan(c), c)).alias(c) for c in economy_df.columns]).show()\n",
    "\n",
    "# Drop Nulls\n",
    "cleaned_economy_df = economy_df.na.drop(subset=['gdp_usd', 'gdp_per_capita_usd'])\n",
    "# df.na.fill({\"age\": 0, \"name\": \"unknown\"}).show()  # Fill missing values\n",
    "\n",
    "print(\"TODO: still need to handle human_capital_index\")\n",
    "\n",
    "print(f\"Reduced {economy_df.count()} rows to {cleaned_economy_df.count()} rows\")\n",
    "\n",
    "print(\"Duplicates\")  # none\n",
    "duplicate_rows = cleaned_economy_df.count() - cleaned_economy_df.dropDuplicates().count()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "cleaned_economy_df = cleaned_economy_df.dropDuplicates()\n",
    "cleaned_economy_df.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-07-14T14:08:44.722-0600\u001b[0m] {\u001b[34mcredentials.py:\u001b[0m621} INFO\u001b[0m - Found credentials in shared credentials file: ~/.aws/credentials\u001b[0m\n",
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close/Last: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+-------+-------+\n",
      "|      Date|Close/Last|   Open|   High|    Low|\n",
      "+----------+----------+-------+-------+-------+\n",
      "|07/12/2024|   5615.35|5590.76|5655.56|5590.44|\n",
      "|07/11/2024|   5584.54|5635.21|5642.45|5576.53|\n",
      "+----------+----------+-------+-------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below: Not in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------------------------+---------------------------+---------------------------+-----------+-----------+---------+-----------------+\n",
      "|      date|location_key|average_temperature_celsius|minimum_temperature_celsius|maximum_temperature_celsius|rainfall_mm|snowfall_mm|dew_point|relative_humidity|\n",
      "+----------+------------+---------------------------+---------------------------+---------------------------+-----------+-----------+---------+-----------------+\n",
      "|2020-01-01|          AD|                   4.236111|                   0.138889|                   8.208333|      3.302|       null|-0.972222|         72.77305|\n",
      "|2020-01-02|          AD|                      3.875|                  -0.722222|                  10.055556|   6.688667|       null|   -1.625|         70.84132|\n",
      "|2020-01-03|          AD|                   4.763889|                   0.597222|                   8.402778|     5.0165|       null|-0.611111|         71.11725|\n",
      "|2020-01-04|          AD|                   4.555556|                      1.125|                   8.708333|       3.81|       null| 0.722222|         77.33864|\n",
      "|2020-01-05|          AD|                   4.763889|                       -1.0|                  11.361111|     2.4765|       null|-3.361111|         60.76238|\n",
      "+----------+------------+---------------------------+---------------------------+---------------------------+-----------+-----------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/GoogleCloudPlatform/covid-19-open-data/blob/main/docs/table-weather.md\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('date', DateType(), True),\n",
    "    StructField('location_key', StringType(), True),\n",
    "    StructField('average_temperature_celsius', FloatType(), True),\n",
    "    StructField('minimum_temperature_celsius', FloatType(), True),\n",
    "    StructField('maximum_temperature_celsius', FloatType(), True),\n",
    "    StructField('rainfall_mm', FloatType(), True),\n",
    "    StructField('snowfall_mm', FloatType(), True),\n",
    "    StructField('dew_point', FloatType(), True),\n",
    "    StructField('relative_humidity', FloatType(), True),\n",
    "])\n",
    "\n",
    "weather_df = spark.read.format(\"csv\").schema(schema).option(\"header\", True).load(COVID_WEATHER_FILE)  # \n",
    "\n",
    "weather_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
