{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pyspark\n",
    "# import matplotlib.pyplot as plt\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType, DateType, DoubleType\n",
    "from pyspark.sql.functions import isnan, isnull, when, count, col, to_date, udf\n",
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "# from airflow.hooks.postgres_hook import PostgresHook\n",
    "# from airflow.hooks.S3_hook import S3Hook\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from constants import COVID_EPIDEMIOLOGY_FILE, COVID_HOSPITALIZATIONS_FILE, SP500_FILE, NASDAQ_FILE, COVID_ECONOMY_FILE, COVID_WEATHER_FILE, \\\n",
    "    RDS_ENDPOINT, RDS_PORT, RDS_USERNAME, RDS_PASSWORD, RDS_DB_NAME, JDBC_DRIVER\n",
    "from aws_helpers import s3_read_to_spark, spark_write_to_rds, spark_read_from_rds\n",
    "from spark_helpers import get_spark_session_and_context\n",
    "\n",
    "# import psycopg2\n",
    "\n",
    "from data_warehouse_helpers import create_date_dim_df, create_company_dim_df\n",
    "\n",
    "# airflow standalone\n",
    "# give ec2 access to s3 using iam roles\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark, sc = get_spark_session_and_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "# print(sc._conf.get('spark.executor.memory'))\n",
    "# spark.conf.get(\"spark.executor.memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-22\n",
      "2021-03-19\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(  # engine\n",
    "    database=RDS_DB_NAME,\n",
    "    user=RDS_USERNAME,\n",
    "    password=RDS_PASSWORD,\n",
    "    host=RDS_ENDPOINT,\n",
    "    port=RDS_PORT\n",
    ")\n",
    "\n",
    "curr = conn.cursor()\n",
    "curr.execute(\"SELECT date FROM sp_five_hundred LIMIT(2);\")\n",
    "\n",
    "rows = curr.fetchall()\n",
    "for row in rows:\n",
    "    print(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close/Last: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      "\n",
      "+----------+----------+-------+-------+-------+\n",
      "|      Date|Close/Last|   Open|   High|    Low|\n",
      "+----------+----------+-------+-------+-------+\n",
      "|07/12/2024|   5615.35|5590.76|5655.56|5590.44|\n",
      "|07/11/2024|   5584.54|5635.21|5642.45|5576.53|\n",
      "|07/10/2024|   5633.91|5591.26|5635.39|5586.44|\n",
      "|07/09/2024|   5576.98|5584.24|5590.75|5574.57|\n",
      "|07/08/2024|   5572.85|5572.75|5583.11|5562.51|\n",
      "+----------+----------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp500_df = s3_read_to_spark(SP500_FILE, spark)  # seemed simpler to read s3 into pandas then spark\n",
    "nasdaq_df = s3_read_to_spark(NASDAQ_FILE, spark)\n",
    "\n",
    "sp500_df.printSchema()\n",
    "sp500_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date format and lowercase all table names\n",
    "def format_stock_df(df):\n",
    "    df = df.withColumn('Date', to_date(col('Date'), 'MM/dd/yyyy')).withColumnRenamed('Date', 'date')  # specify the given date\n",
    "    df = df.withColumnRenamed('Close/Last', 'close')\n",
    "    df = df.withColumnRenamed('Open', 'open')\n",
    "    df = df.withColumnRenamed('High', 'high')\n",
    "    df = df.withColumnRenamed('Low', 'low')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sp500_df = format_stock_df(sp500_df)\n",
    "nasdaq_df = format_stock_df(nasdaq_df)\n",
    "\n",
    "spark_write_to_rds(sp500_df, table_name=\"sp_five_hundred\")\n",
    "spark_write_to_rds(nasdaq_df, table_name=\"nasdaq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq_companies_file = \"data/stocks/nasdaq_companies.csv\"\n",
    "nasdaq_companies_df = spark.read.option('header', True).csv(nasdaq_companies_file)\n",
    "nasdaq_companies_df.createOrReplaceTempView(\"nasdaq_companies\")\n",
    "\n",
    "\n",
    "sp500_df = spark_read_from_rds(spark, \"sp_five_hundred\")\n",
    "sp500_df.createOrReplaceTempView('sp_five_hundred')\n",
    "nasdaq_df = spark_read_from_rds(spark, \"nasdaq\")\n",
    "nasdaq_df.createOrReplaceTempView('nasdaq')\n",
    "# nasdaq_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------------+-------------+--------+-------+-----------+--------------------+\n",
      "|company_id|symbol|                name|      country|ipo_year| volume|     sector|            industry|\n",
      "+----------+------+--------------------+-------------+--------+-------+-----------+--------------------+\n",
      "|         1|     A|Agilent Technolog...|United States|    1999|1064325|Industrials|Biotechnology: La...|\n",
      "|         2|    AA|Alcoa Corporation...|United States|    2016|6201684|Industrials|            Aluminum|\n",
      "|         3|  AACG|ATA Creativity Gl...|        China|    2008| 203999|Real Estate|Other Consumer Se...|\n",
      "|         4|  AACI|Armada Acquisitio...|United States|    2021|  18159|    Finance|        Blank Checks|\n",
      "|         5| AACIU|Armada Acquisitio...|United States|    2021|    244|    Finance|        Blank Checks|\n",
      "+----------+------+--------------------+-------------+--------+-------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/16 22:50:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/16 22:50:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/16 22:50:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "nasdaq_companies_df = create_company_dim_df(spark)\n",
    "nasdaq_companies_df.show(5)\n",
    "spark_write_to_rds(nasdaq_companies_df, \"company_dim\")\n",
    "# nasdaq_companies_df.createOrReplaceTempView(\"nasdaq_companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/16 22:51:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/16 22:51:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/16 22:51:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/16 22:51:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/16 22:51:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/16 22:51:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/16 22:51:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/16 22:51:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---+-------------+-----+---------------+----+-------+\n",
      "|date_id|      date|day|day_name_abbr|month|month_name_abbr|year|quarter|\n",
      "+-------+----------+---+-------------+-----+---------------+----+-------+\n",
      "|      1|2018-01-01|  1|          Mon|    1|            Jan|2018|      1|\n",
      "|      2|2018-01-02|  2|          Tue|    1|            Jan|2018|      1|\n",
      "|      3|2018-01-03|  3|          Wed|    1|            Jan|2018|      1|\n",
      "|      4|2018-01-04|  4|          Thu|    1|            Jan|2018|      1|\n",
      "|      5|2018-01-05|  5|          Fri|    1|            Jan|2018|      1|\n",
      "+-------+----------+---+-------------+-----+---------------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df = create_date_dim_df(spark)\n",
    "spark_write_to_rds(date_df, \"date_dim\")\n",
    "date_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"\n",
    "#     SELECT\n",
    "#     FROM nasdaq\n",
    "#     LEFT JOIN nasdaq_companies ON symbol\n",
    "# \"\"\")\n",
    "## i realized we only have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "+--------+------+----------+-----+-----+-----+-----+---------+-------+\n",
      "|price_id|symbol|      date| open| high|  low|close|adj_close| volume|\n",
      "+--------+------+----------+-----+-----+-----+-----+---------+-------+\n",
      "|       1|     A|2018-01-02|67.42|67.89|67.34| 67.6|64.401215|1047800|\n",
      "|       2|     A|2018-01-03|67.62|69.49| 67.6|69.32|66.039856|1698900|\n",
      "|       3|     A|2018-01-04|69.54|69.82|68.78| 68.8|65.544426|2230700|\n",
      "|       4|     A|2018-01-05|68.73| 70.1|68.73| 69.9|  66.5924|1632500|\n",
      "|       5|     A|2018-01-08|69.73|70.33|69.55|70.05|66.735306|1613400|\n",
      "+--------+------+----------+-----+-----+-----+-----+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/17 00:25:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/17 00:25:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/17 00:25:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/17 00:25:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/17 00:25:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/17 00:25:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/17 00:25:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/17 00:25:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+------+------+------+------+---------+-------+\n",
      "|price_id|symbol|      date|  open|  high|   low| close|adj_close| volume|\n",
      "+--------+------+----------+------+------+------+------+---------+-------+\n",
      "|    1509|     A|2023-12-29|139.07| 139.7|138.36|139.03|138.54736|1014400|\n",
      "|    1508|     A|2023-12-28|140.54|140.81|139.65|139.77|139.04964| 892600|\n",
      "|    1507|     A|2023-12-27|139.78|140.16|139.08|139.82| 139.0994|1182300|\n",
      "|    1506|     A|2023-12-26|139.31|140.47|139.09|139.81|139.08943| 948400|\n",
      "|    1505|     A|2023-12-22|139.61|140.36|138.79|139.57|138.85066|1204100|\n",
      "+--------+------+----------+------+------+------+------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/17 00:25:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/17 00:25:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/17 00:25:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/17 00:25:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/17 00:25:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1509\n",
      "+--------+------+----------+-----+-----+------+-----+---------+-------+\n",
      "|price_id|symbol|      date| open| high|   low|close|adj_close| volume|\n",
      "+--------+------+----------+-----+-----+------+-----+---------+-------+\n",
      "|    1510|    AA|2018-01-02|54.06|55.22| 53.91|55.17|53.676037|2928900|\n",
      "|    1511|    AA|2018-01-03|54.92|55.15| 52.96| 54.5| 53.02418|4100000|\n",
      "|    1512|    AA|2018-01-04|54.81|55.43|54.075| 54.7|53.218773|3555100|\n",
      "|    1513|    AA|2018-01-05|54.65|54.66| 53.41|54.09| 52.62529|3371400|\n",
      "|    1514|    AA|2018-01-08|53.96|56.15| 53.66| 55.0| 53.51064|5028100|\n",
      "+--------+------+----------+-----+-----+------+-----+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/17 00:25:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/17 00:25:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/17 00:25:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/17 00:25:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/17 00:25:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/17 00:25:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+-----+------+------+-----+---------+-------+\n",
      "|price_id|symbol|      date| open|  high|   low|close|adj_close| volume|\n",
      "+--------+------+----------+-----+------+------+-----+---------+-------+\n",
      "|    3018|    AA|2023-12-29|34.31| 34.74| 33.93| 34.0| 33.79498|3294300|\n",
      "|    3017|    AA|2023-12-28|34.68|34.812|34.198|34.55|34.341667|3909900|\n",
      "|    3016|    AA|2023-12-27|34.23|35.035| 33.75|34.81|34.600098|5798100|\n",
      "|    3015|    AA|2023-12-26|33.86| 34.12|  33.5|33.87|33.665768|4505900|\n",
      "|    3014|    AA|2023-12-22|32.96| 34.44| 32.88|33.77| 33.56637|9064600|\n",
      "+--------+------+----------+-----+------+------+-----+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/17 00:25:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/07/17 00:25:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField('symbol', StringType(), True),\n",
    "    StructField('date', DateType(), True),\n",
    "    StructField('open', FloatType(), True),\n",
    "    StructField('high', DoubleType(), True),\n",
    "    StructField('low', DoubleType(), True),\n",
    "    StructField('close', DoubleType(), True),\n",
    "    StructField('adj_close', DoubleType(), True),\n",
    "    StructField('volume', DoubleType(), True)\n",
    "])\n",
    "\n",
    "# stock_prices_df = spark.createDataFrame()\n",
    "indiv_stocks_path = 'data/stocks/indiv_stocks'\n",
    "\n",
    "company_files = sorted(os.listdir(indiv_stocks_path))\n",
    "data = []\n",
    "\n",
    "# df = df.withColumn(\n",
    "#   \"num_id\", F.row_number().over(Window.orderBy( \n",
    "#               F.monotonically_increasing_id()))) \n",
    "\n",
    "from constants import START_DATE, END_DATE\n",
    "\n",
    "table_name = 'stock_prices'\n",
    "\n",
    "# raw_col_names = \n",
    "last_idx = 0\n",
    "for file_name in company_files:\n",
    "    symbol = file_name.rstrip('.csv')\n",
    "\n",
    "    file_path = f\"{indiv_stocks_path}/{file_name}\"\n",
    "    temp_df = spark.read.option('header', True).csv(file_path)\n",
    "\n",
    "    # lowercase all names\n",
    "    for column in temp_df.columns:\n",
    "        new_name = column.lower()\n",
    "        temp_df = temp_df.withColumnRenamed(column, new_name)\n",
    "    temp_df = temp_df.withColumnRenamed('adj close', 'adj_close')  # replace with underscore\n",
    "    temp_df = temp_df.withColumn('date', temp_df['date'].cast('date'))\n",
    "\n",
    "    temp_df = temp_df.where((F.col('date') >= F.lit(START_DATE)) & (F.col('date') <= F.lit(END_DATE))).select(\"*\")\n",
    "\n",
    "\n",
    "    # cast to float\n",
    "    for col in ['open', 'high', 'low', 'close', 'adj_close']:\n",
    "        temp_df = temp_df.withColumn(col, temp_df[col].cast('float'))\n",
    "    temp_df = temp_df.withColumn('symbol', F.lit(symbol))\n",
    "\n",
    "    temp_df = temp_df.withColumn('price_id', last_idx + F.row_number().over(Window().orderBy('date')))\n",
    "    temp_df = temp_df.select('price_id', 'symbol', 'date', 'open', 'high', 'low', 'close', 'adj_close', 'volume')\n",
    "\n",
    "    print(last_idx)\n",
    "    temp_df.show(5)\n",
    "    temp_df.orderBy(F.desc('date')).show(5)\n",
    "    # last_idx = temp_df.select(F.last('price_id')).head()[0]\n",
    "    # last_idx = temp_df.['price_id']\n",
    "    # print(temp_df.orderBy(F.desc('price_id')).show(5))  # show(5)\n",
    "    # if next_idx == 1:\n",
    "    #     spark_write_to_rds(temp_df, table_name)\n",
    "    # else:\n",
    "    #     temp_df.write.insertInto(tableName=table_name)\n",
    "    if last_idx != 0:\n",
    "        break\n",
    "    last_idx = temp_df.select('price_id').tail(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitalizations_df = s3_read_to_spark(COVID_HOSPITALIZATIONS_FILE, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/16 08:03:32 WARN TaskSetManager: Stage 6 contains a task of very large size (14155 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------------------+--------------------------------+-----------------------------+---------------------------+----------------------------------+-------------------------------+-----------------------+------------------------------+---------------------------+\n",
      "|      date|location_key|new_hospitalized_patients|cumulative_hospitalized_patients|current_hospitalized_patients|new_intensive_care_patients|cumulative_intensive_care_patients|current_intensive_care_patients|new_ventilator_patients|cumulative_ventilator_patients|current_ventilator_patients|\n",
      "+----------+------------+-------------------------+--------------------------------+-----------------------------+---------------------------+----------------------------------+-------------------------------+-----------------------+------------------------------+---------------------------+\n",
      "|0022-01-10|          AR|                      0.0|                             0.0|                          NaN|                        0.0|                               0.0|                            NaN|                    NaN|                           NaN|                        NaN|\n",
      "|0022-01-20|          AR|                      0.0|                             0.0|                          NaN|                        0.0|                               0.0|                            NaN|                    NaN|                           NaN|                        NaN|\n",
      "|0202-01-30|          AR|                      0.0|                             0.0|                          NaN|                        0.0|                               0.0|                            NaN|                    NaN|                           NaN|                        NaN|\n",
      "|0221-01-06|          AR|                      0.0|                             0.0|                          NaN|                        0.0|                               0.0|                            NaN|                    NaN|                           NaN|                        NaN|\n",
      "|1202-01-07|          AR|                      0.0|                             0.0|                          NaN|                        0.0|                               0.0|                            NaN|                    NaN|                           NaN|                        NaN|\n",
      "+----------+------------+-------------------------+--------------------------------+-----------------------------+---------------------------+----------------------------------+-------------------------------+-----------------------+------------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/16 08:03:36 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 6 (TID 6): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('date', 'date'),\n",
       " ('location_key', 'string'),\n",
       " ('new_hospitalized_patients', 'double'),\n",
       " ('cumulative_hospitalized_patients', 'double'),\n",
       " ('current_hospitalized_patients', 'double'),\n",
       " ('new_intensive_care_patients', 'double'),\n",
       " ('cumulative_intensive_care_patients', 'double'),\n",
       " ('current_intensive_care_patients', 'double'),\n",
       " ('new_ventilator_patients', 'double'),\n",
       " ('cumulative_ventilator_patients', 'double'),\n",
       " ('current_ventilator_patients', 'double')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospitalizations_df = hospitalizations_df.withColumn('date', to_date(col('date'), 'yyyy-mm-dd'))\n",
    "hospitalizations_df.show(5)\n",
    "hospitalizations_df.dtypes\n",
    "# spark_write_to_rds(hospitalizations_df, table_name=\"hospitalizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/16 08:06:40 WARN TaskSetManager: Stage 9 contains a task of very large size (14155 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_write_to_rds(hospitalizations_df, table_name=\"hospitalizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('header', True).csv('data/covid/us_location.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(col('location_key'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%configure\n",
    "# # {\"conf\": {\"spark.jars.packages\": \"org.mongodb.spark:mongo-spark-connector_2.11:2.3.2\"}}\n",
    "# data_file = \"data/covid/epidemiology.json\"\n",
    "# temp_df = spark.read.json(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymongo\n",
    "from constants import MONGO_URI\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "client = MongoClient(MONGO_URI, server_api=ServerApi('1'))\n",
    "\n",
    "covid_db = client['covid_pipeline']\n",
    "\n",
    "epid_collection = covid_db['epidemiology']\n",
    "\n",
    "\n",
    "import sys\n",
    "data_file = \"data/covid/epidemiology.json\"\n",
    "with open(data_file, \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "print(sys.getsizeof(json_data))\n",
    "\n",
    "columns, data = json_data['columns'], json_data['data']\n",
    "print(columns)\n",
    "documents = []\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "for entry in data:\n",
    "    d = {col: entry[i] for i, col in enumerate(columns)}\n",
    "    documents.append(d)\n",
    "\n",
    "x = epid_collection.insert_many(documents)\n",
    "client.close()\n",
    "\n",
    "\n",
    "# spark.read.format('mongodb').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sp500_df\n",
    "\n",
    "print(\"null count\")\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# print(\"nan count\")  # none\n",
    "# df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Drop Nulls\n",
    "cleaned_df = df.na.drop()  # subset=['gdp_usd', 'gdp_per_capita_usd']\n",
    "# df.na.fill({\"age\": 0, \"name\": \"unknown\"}).show()  # Fill missing values\n",
    "\n",
    "\n",
    "for col_name in ['new_confirmed', 'new_deceased', 'new_recovered', 'new_tested']:\n",
    "    print(f\"Changing {cleaned_df.where(col(col_name) < 0).select(col(col_name)).count()}, negative values in {col_name} to 0\")\n",
    "\n",
    "    cleaned_df = cleaned_df.withColumn(col_name, when(col(col_name) < 0, 0).otherwise(col(col_name)))\n",
    "\n",
    "# print(\"Duplicates\")  # none\n",
    "duplicate_rows = cleaned_df.count() - cleaned_df.dropDuplicates().count()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "cleaned_df = cleaned_df.dropDuplicates()\n",
    "cleaned_df.limit(10).show()\n",
    "print(f\"Reduced {df.count()} rows to {cleaned_df.count()} rows\")\n",
    "claned_epidemiology_df = cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|      date|location_key|new_confirmed|new_deceased|new_recovered|new_tested|cumulative_confirmed|cumulative_deceased|cumulative_recovered|cumulative_tested|\n",
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|2020-01-01|          AD|            0|           0|         null|      null|                   0|                  0|                null|             null|\n",
      "|2020-01-02|          AD|            0|           0|         null|      null|                   0|                  0|                null|             null|\n",
      "|2020-01-03|          AD|            0|           0|         null|      null|                   0|                  0|                null|             null|\n",
      "|2020-01-04|          AD|            0|           0|         null|      null|                   0|                  0|                null|             null|\n",
      "|2020-01-05|          AD|            0|           0|         null|      null|                   0|                  0|                null|             null|\n",
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/GoogleCloudPlatform/covid-19-open-data/blob/main/docs/table-epidemiology.md\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('date', DateType(), True),\n",
    "    StructField('location_key', StringType(), True),\n",
    "    StructField('new_confirmed', IntegerType(), True),\n",
    "    StructField('new_deceased', IntegerType(), True),\n",
    "    StructField('new_recovered', IntegerType(), True),\n",
    "    StructField('new_tested', IntegerType(), True),\n",
    "    StructField('cumulative_confirmed', IntegerType(), True),\n",
    "    StructField('cumulative_deceased', IntegerType(), True),\n",
    "    StructField('cumulative_recovered', IntegerType(), True),\n",
    "    StructField('cumulative_tested', IntegerType(), True)\n",
    "])\n",
    "\n",
    "epidemiology_df = spark.read.format(\"csv\").schema(schema).option(\"header\", True).load(COVID_EPIDEMIOLOGY_FILE)\n",
    "\n",
    "epidemiology_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|date|location_key|new_confirmed|new_deceased|new_recovered|new_tested|cumulative_confirmed|cumulative_deceased|cumulative_recovered|cumulative_tested|\n",
      "+----+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|   0|           0|        50025|      858687|      8545363|   9331336|              198780|            1051000|             8534668|          9512906|\n",
      "+----+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing 26453, negative values in new_confirmed to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing 6986, negative values in new_deceased to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing 1888, negative values in new_recovered to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing 2769, negative values in new_tested to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|      date|location_key|new_confirmed|new_deceased|new_recovered|new_tested|cumulative_confirmed|cumulative_deceased|cumulative_recovered|cumulative_tested|\n",
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|2020-12-23|          AT|         2843|          99|         3115|     38440|              344938|               6994|              307537|          3682136|\n",
      "|2021-02-07|          AT|          993|          40|         1052|    212647|              420176|               9780|              391279|         10313170|\n",
      "|2021-06-08|          AT|          356|           4|          607|    412011|              641360|              12964|              620881|         45983005|\n",
      "|2021-08-15|          AT|          797|           1|          432|    395705|              663338|              13154|              638777|         71577449|\n",
      "|2021-10-02|          AT|         1555|          15|         1785|    225990|              742938|              13522|              702140|         87701064|\n",
      "|2022-05-26|          AT|         1736|           7|         2924|    259173|             4265951|              19893|             4219052|        187227485|\n",
      "|2021-04-25|        AT_1|           54|           0|           49|      6199|               17290|                384|               16213|          1254623|\n",
      "|2021-11-23|        AT_1|          340|           0|          347|      5140|               28100|                457|               23923|          3388215|\n",
      "|2021-11-01|        AT_2|          350|           1|          184|      5829|               49890|               1110|               45898|          4801002|\n",
      "|2022-04-09|        AT_2|          512|           2|         1155|      1547|              234624|               1588|              226771|          6489938|\n",
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 474:=============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced 12525825 rows to 2450433 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = epidemiology_df\n",
    "\n",
    "print(\"null count\")\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# print(\"nan count\")  # none\n",
    "# df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Drop Nulls\n",
    "cleaned_df = df.na.drop()  # subset=['gdp_usd', 'gdp_per_capita_usd']\n",
    "# df.na.fill({\"age\": 0, \"name\": \"unknown\"}).show()  # Fill missing values\n",
    "\n",
    "\n",
    "for col_name in ['new_confirmed', 'new_deceased', 'new_recovered', 'new_tested']:\n",
    "    print(f\"Changing {cleaned_df.where(col(col_name) < 0).select(col(col_name)).count()}, negative values in {col_name} to 0\")\n",
    "\n",
    "    cleaned_df = cleaned_df.withColumn(col_name, when(col(col_name) < 0, 0).otherwise(col(col_name)))\n",
    "\n",
    "# print(\"Duplicates\")  # none\n",
    "duplicate_rows = cleaned_df.count() - cleaned_df.dropDuplicates().count()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "cleaned_df = cleaned_df.dropDuplicates()\n",
    "cleaned_df.limit(10).show()\n",
    "print(f\"Reduced {df.count()} rows to {cleaned_df.count()} rows\")\n",
    "claned_epidemiology_df = cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+-------------------+\n",
      "|location_key|     gdp_usd|gdp_per_capita_usd|human_capital_index|\n",
      "+------------+------------+------------------+-------------------+\n",
      "|          AD|  3154057987|             40886|               null|\n",
      "|          AE|421142267937|             43103|              0.659|\n",
      "|          AF| 19101353832|               502|              0.389|\n",
      "|          AG|  1727759259|             17790|               null|\n",
      "|          AL| 15278077446|              5352|              0.621|\n",
      "+------------+------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/GoogleCloudPlatform/covid-19-open-data/blob/main/docs/table-economy.md\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('location_key', StringType(), True),\n",
    "    StructField('gdp_usd', LongType(), True),\n",
    "    StructField('gdp_per_capita_usd', IntegerType(), True),\n",
    "    StructField('human_capital_index', FloatType(), True)\n",
    "])\n",
    "\n",
    "economy_df = spark.read.format(\"csv\").schema(schema).option(\"header\", True).load(COVID_ECONOMY_FILE)\n",
    "\n",
    "economy_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null count\n",
      "+------------+-------+------------------+-------------------+\n",
      "|location_key|gdp_usd|gdp_per_capita_usd|human_capital_index|\n",
      "+------------+-------+------------------+-------------------+\n",
      "|           0|     31|                39|                248|\n",
      "+------------+-------+------------------+-------------------+\n",
      "\n",
      "TODO: still need to handle human_capital_index\n",
      "Reduced 404 rows to 334 rows\n",
      "Duplicates\n",
      "Number of duplicate rows: 0\n",
      "+------------+------------+------------------+-------------------+\n",
      "|location_key|     gdp_usd|gdp_per_capita_usd|human_capital_index|\n",
      "+------------+------------+------------------+-------------------+\n",
      "|       DE_ST| 73969539000|             33394|               null|\n",
      "|       IT_25|457916381400|             45548|               null|\n",
      "|        AT_5| 34272274000|             61832|               null|\n",
      "|       IT_88| 41212125400|             25016|               null|\n",
      "|          GN| 13590281808|              1064|              0.374|\n",
      "|      BE_BRU| 99104176200|             81892|               null|\n",
      "|      BE_WNA| 15884452000|             31978|               null|\n",
      "|          CL|282318159744|             14896|              0.674|\n",
      "|       NL_GR| 29455974200|             50504|               null|\n",
      "|       NL_ZE| 16022665400|             41890|               null|\n",
      "+------------+------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"null count\")\n",
    "economy_df.select([count(when(isnull(c), c)).alias(c) for c in economy_df.columns]).show()\n",
    "# print(\"nan count\")  # none\n",
    "# economy_df.select([count(when(isnan(c), c)).alias(c) for c in economy_df.columns]).show()\n",
    "\n",
    "# Drop Nulls\n",
    "cleaned_economy_df = economy_df.na.drop(subset=['gdp_usd', 'gdp_per_capita_usd'])\n",
    "# df.na.fill({\"age\": 0, \"name\": \"unknown\"}).show()  # Fill missing values\n",
    "\n",
    "print(\"TODO: still need to handle human_capital_index\")\n",
    "\n",
    "print(f\"Reduced {economy_df.count()} rows to {cleaned_economy_df.count()} rows\")\n",
    "\n",
    "print(\"Duplicates\")  # none\n",
    "duplicate_rows = cleaned_economy_df.count() - cleaned_economy_df.dropDuplicates().count()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "cleaned_economy_df = cleaned_economy_df.dropDuplicates()\n",
    "cleaned_economy_df.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-07-14T14:08:44.722-0600\u001b[0m] {\u001b[34mcredentials.py:\u001b[0m621} INFO\u001b[0m - Found credentials in shared credentials file: ~/.aws/credentials\u001b[0m\n",
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close/Last: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+-------+-------+\n",
      "|      Date|Close/Last|   Open|   High|    Low|\n",
      "+----------+----------+-------+-------+-------+\n",
      "|07/12/2024|   5615.35|5590.76|5655.56|5590.44|\n",
      "|07/11/2024|   5584.54|5635.21|5642.45|5576.53|\n",
      "+----------+----------+-------+-------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below: Not in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------------------------+---------------------------+---------------------------+-----------+-----------+---------+-----------------+\n",
      "|      date|location_key|average_temperature_celsius|minimum_temperature_celsius|maximum_temperature_celsius|rainfall_mm|snowfall_mm|dew_point|relative_humidity|\n",
      "+----------+------------+---------------------------+---------------------------+---------------------------+-----------+-----------+---------+-----------------+\n",
      "|2020-01-01|          AD|                   4.236111|                   0.138889|                   8.208333|      3.302|       null|-0.972222|         72.77305|\n",
      "|2020-01-02|          AD|                      3.875|                  -0.722222|                  10.055556|   6.688667|       null|   -1.625|         70.84132|\n",
      "|2020-01-03|          AD|                   4.763889|                   0.597222|                   8.402778|     5.0165|       null|-0.611111|         71.11725|\n",
      "|2020-01-04|          AD|                   4.555556|                      1.125|                   8.708333|       3.81|       null| 0.722222|         77.33864|\n",
      "|2020-01-05|          AD|                   4.763889|                       -1.0|                  11.361111|     2.4765|       null|-3.361111|         60.76238|\n",
      "+----------+------------+---------------------------+---------------------------+---------------------------+-----------+-----------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/GoogleCloudPlatform/covid-19-open-data/blob/main/docs/table-weather.md\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('date', DateType(), True),\n",
    "    StructField('location_key', StringType(), True),\n",
    "    StructField('average_temperature_celsius', FloatType(), True),\n",
    "    StructField('minimum_temperature_celsius', FloatType(), True),\n",
    "    StructField('maximum_temperature_celsius', FloatType(), True),\n",
    "    StructField('rainfall_mm', FloatType(), True),\n",
    "    StructField('snowfall_mm', FloatType(), True),\n",
    "    StructField('dew_point', FloatType(), True),\n",
    "    StructField('relative_humidity', FloatType(), True),\n",
    "])\n",
    "\n",
    "weather_df = spark.read.format(\"csv\").schema(schema).option(\"header\", True).load(COVID_WEATHER_FILE)  # \n",
    "\n",
    "weather_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
