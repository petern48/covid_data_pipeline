{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install matplotlib\n",
    "# pip install pyspark[sql]\n",
    "# ipython package fo jupyter notebooks\n",
    "\n",
    "# easy spark install using conda\n",
    "# https://medium.com/the-ai-guide/easy-install-pyspark-in-anaconda-e2d427b3492f\n",
    "\n",
    "# for installing java, scala for spark. actually don't use this\n",
    "# https://medium.com/@aitmsi/single-node-spark-pyspark-cluster-on-windows-subsystem-for-linux-wsl2-22860888a98d\n",
    "# maybe i should uninstall this\n",
    "# $ sudo apt update && upgrade\n",
    "# $ sudo apt-get install openjdk-11-jre\n",
    "# $ sudo apt-get install openjdk-11-jdk\n",
    "\n",
    "# for installing airflow\n",
    "# https://sebastien-sime.medium.com/installing-apache-airflow-on-windows-10-wsl-314da1b28f3b\n",
    "# added a /etc/wsl.conf file and filled it according to^\n",
    "# pip3 install apache-airflow==1.10.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipykernel_1673/3500185117.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">9</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> DeprecationWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: The `airflow.operators.python_operator.PythonOperator` class is deprecated. Please use `</span><span style=\"color: #808000; text-decoration-color: #808000\">'airflow.operators.python.PythonOperator'</span><span style=\"color: #808000; text-decoration-color: #808000\">`.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33m/tmp/ipykernel_1673/\u001b[0m\u001b[1;33m3500185117.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m9\u001b[0m\u001b[1;33m DeprecationWarning\u001b[0m\u001b[33m: The `airflow.operators.python_operator.PythonOperator` class is deprecated. Please use `\u001b[0m\u001b[33m'airflow.operators.python.PythonOperator'\u001b[0m\u001b[33m`.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark\n",
    "# import matplotlib.pyplot as plt\n",
    "from pyspark import SparkContext  # , SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType, DateType\n",
    "from pyspark.sql.functions import isnan, isnull, when, count, col\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "# from airflow.hooks.postgres_hook import PostgresHook\n",
    "# from airflow.hooks.S3_hook import S3Hook\n",
    "# import pandas as pd\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "import re\n",
    "# import psycopg2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a spark context class\n",
    "# sc = SparkContext()\n",
    "\n",
    "# Creating a spark session\n",
    "spark = SparkSession.builder.appName('pySparkSetup').getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "DATA_DIR = \"data\"\n",
    "COVID_ECONOMY_FILE = f\"{DATA_DIR}/economy.csv\"\n",
    "COVID_EPIDEMIOLOGY_FILE = f\"{DATA_DIR}/epidemiology.csv\"\n",
    "COVID_WEATHER_FILE = f\"{DATA_DIR}/weather.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+-------------------+\n",
      "|location_key|     gdp_usd|gdp_per_capita_usd|human_capital_index|\n",
      "+------------+------------+------------------+-------------------+\n",
      "|          AD|  3154057987|             40886|               null|\n",
      "|          AE|421142267937|             43103|              0.659|\n",
      "|          AF| 19101353832|               502|              0.389|\n",
      "|          AG|  1727759259|             17790|               null|\n",
      "|          AL| 15278077446|              5352|              0.621|\n",
      "+------------+------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/GoogleCloudPlatform/covid-19-open-data/blob/main/docs/table-economy.md\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('location_key', StringType(), True),\n",
    "    StructField('gdp_usd', LongType(), True),\n",
    "    StructField('gdp_per_capita_usd', IntegerType(), True),\n",
    "    StructField('human_capital_index', FloatType(), True)\n",
    "])\n",
    "\n",
    "economy_df = spark.read.format(\"csv\").schema(schema).option(\"header\", True).load(COVID_ECONOMY_FILE)\n",
    "\n",
    "economy_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null count\n",
      "+------------+-------+------------------+-------------------+\n",
      "|location_key|gdp_usd|gdp_per_capita_usd|human_capital_index|\n",
      "+------------+-------+------------------+-------------------+\n",
      "|           0|     31|                39|                248|\n",
      "+------------+-------+------------------+-------------------+\n",
      "\n",
      "TODO: still need to handle human_capital_index\n",
      "Reduced 404 rows to 334 rows\n",
      "Duplicates\n",
      "Number of duplicate rows: 0\n",
      "+------------+------------+------------------+-------------------+\n",
      "|location_key|     gdp_usd|gdp_per_capita_usd|human_capital_index|\n",
      "+------------+------------+------------------+-------------------+\n",
      "|       DE_ST| 73969539000|             33394|               null|\n",
      "|       IT_25|457916381400|             45548|               null|\n",
      "|        AT_5| 34272274000|             61832|               null|\n",
      "|       IT_88| 41212125400|             25016|               null|\n",
      "|          GN| 13590281808|              1064|              0.374|\n",
      "|      BE_BRU| 99104176200|             81892|               null|\n",
      "|      BE_WNA| 15884452000|             31978|               null|\n",
      "|          CL|282318159744|             14896|              0.674|\n",
      "|       NL_GR| 29455974200|             50504|               null|\n",
      "|       NL_ZE| 16022665400|             41890|               null|\n",
      "+------------+------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"null count\")\n",
    "economy_df.select([count(when(isnull(c), c)).alias(c) for c in economy_df.columns]).show()\n",
    "# print(\"nan count\")  # none\n",
    "# economy_df.select([count(when(isnan(c), c)).alias(c) for c in economy_df.columns]).show()\n",
    "\n",
    "# Drop Nulls\n",
    "cleaned_economy_df = economy_df.na.drop(subset=['gdp_usd', 'gdp_per_capita_usd'])\n",
    "\n",
    "print(\"TODO: still need to handle human_capital_index\")\n",
    "\n",
    "print(f\"Reduced {economy_df.count()} rows to {cleaned_economy_df.count()} rows\")\n",
    "\n",
    "print(\"Duplicates\")  # none\n",
    "duplicate_rows = cleaned_economy_df.count() - cleaned_economy_df.dropDuplicates().count()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "cleaned_economy_df = cleaned_economy_df.dropDuplicates()\n",
    "cleaned_economy_df.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|      date|location_key|new_confirmed|new_deceased|new_recovered|new_tested|cumulative_confirmed|cumulative_deceased|cumulative_recovered|cumulative_tested|\n",
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|2020-01-01|          AD|            0|           0|         null|      null|                   0|                  0|                null|             null|\n",
      "|2020-01-02|          AD|            0|           0|         null|      null|                   0|                  0|                null|             null|\n",
      "|2020-01-03|          AD|            0|           0|         null|      null|                   0|                  0|                null|             null|\n",
      "|2020-01-04|          AD|            0|           0|         null|      null|                   0|                  0|                null|             null|\n",
      "|2020-01-05|          AD|            0|           0|         null|      null|                   0|                  0|                null|             null|\n",
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/GoogleCloudPlatform/covid-19-open-data/blob/main/docs/table-epidemiology.md\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('date', DateType(), True),\n",
    "    StructField('location_key', StringType(), True),\n",
    "    StructField('new_confirmed', IntegerType(), True),\n",
    "    StructField('new_deceased', IntegerType(), True),\n",
    "    StructField('new_recovered', IntegerType(), True),\n",
    "    StructField('new_tested', IntegerType(), True),\n",
    "    StructField('cumulative_confirmed', IntegerType(), True),\n",
    "    StructField('cumulative_deceased', IntegerType(), True),\n",
    "    StructField('cumulative_recovered', IntegerType(), True),\n",
    "    StructField('cumulative_tested', IntegerType(), True)\n",
    "])\n",
    "\n",
    "epidemiology_df = spark.read.format(\"csv\").schema(schema).option(\"header\", True).load(COVID_EPIDEMIOLOGY_FILE)\n",
    "\n",
    "epidemiology_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|date|location_key|new_confirmed|new_deceased|new_recovered|new_tested|cumulative_confirmed|cumulative_deceased|cumulative_recovered|cumulative_tested|\n",
      "+----+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|   0|           0|        50025|      858687|      8545363|   9331336|              198780|            1051000|             8534668|          9512906|\n",
      "+----+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing 26453, negative values in new_confirmed to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing 6986, negative values in new_deceased to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing 1888, negative values in new_recovered to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing 2769, negative values in new_tested to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|      date|location_key|new_confirmed|new_deceased|new_recovered|new_tested|cumulative_confirmed|cumulative_deceased|cumulative_recovered|cumulative_tested|\n",
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "|2020-12-23|          AT|         2843|          99|         3115|     38440|              344938|               6994|              307537|          3682136|\n",
      "|2021-02-07|          AT|          993|          40|         1052|    212647|              420176|               9780|              391279|         10313170|\n",
      "|2021-06-08|          AT|          356|           4|          607|    412011|              641360|              12964|              620881|         45983005|\n",
      "|2021-08-15|          AT|          797|           1|          432|    395705|              663338|              13154|              638777|         71577449|\n",
      "|2021-10-02|          AT|         1555|          15|         1785|    225990|              742938|              13522|              702140|         87701064|\n",
      "|2022-05-26|          AT|         1736|           7|         2924|    259173|             4265951|              19893|             4219052|        187227485|\n",
      "|2021-04-25|        AT_1|           54|           0|           49|      6199|               17290|                384|               16213|          1254623|\n",
      "|2021-11-23|        AT_1|          340|           0|          347|      5140|               28100|                457|               23923|          3388215|\n",
      "|2021-11-01|        AT_2|          350|           1|          184|      5829|               49890|               1110|               45898|          4801002|\n",
      "|2022-04-09|        AT_2|          512|           2|         1155|      1547|              234624|               1588|              226771|          6489938|\n",
      "+----------+------------+-------------+------------+-------------+----------+--------------------+-------------------+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 474:=============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced 12525825 rows to 2450433 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = epidemiology_df\n",
    "\n",
    "print(\"null count\")\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# print(\"nan count\")  # none\n",
    "# df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Drop Nulls\n",
    "cleaned_df = df.na.drop()  # subset=['gdp_usd', 'gdp_per_capita_usd']\n",
    "\n",
    "\n",
    "for col_name in ['new_confirmed', 'new_deceased', 'new_recovered', 'new_tested']:\n",
    "    print(f\"Changing {cleaned_df.where(col(col_name) < 0).select(col(col_name)).count()}, negative values in {col_name} to 0\")\n",
    "\n",
    "    cleaned_df = cleaned_df.withColumn(col_name, when(col(col_name) < 0, 0).otherwise(col(col_name)))\n",
    "\n",
    "# print(\"Duplicates\")  # none\n",
    "duplicate_rows = cleaned_df.count() - cleaned_df.dropDuplicates().count()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "cleaned_df = cleaned_df.dropDuplicates()\n",
    "cleaned_df.limit(10).show()\n",
    "print(f\"Reduced {df.count()} rows to {cleaned_df.count()} rows\")\n",
    "claned_epidemiology_df = cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------------------------+---------------------------+---------------------------+-----------+-----------+---------+-----------------+\n",
      "|      date|location_key|average_temperature_celsius|minimum_temperature_celsius|maximum_temperature_celsius|rainfall_mm|snowfall_mm|dew_point|relative_humidity|\n",
      "+----------+------------+---------------------------+---------------------------+---------------------------+-----------+-----------+---------+-----------------+\n",
      "|2020-01-01|          AD|                   4.236111|                   0.138889|                   8.208333|      3.302|       null|-0.972222|         72.77305|\n",
      "|2020-01-02|          AD|                      3.875|                  -0.722222|                  10.055556|   6.688667|       null|   -1.625|         70.84132|\n",
      "|2020-01-03|          AD|                   4.763889|                   0.597222|                   8.402778|     5.0165|       null|-0.611111|         71.11725|\n",
      "|2020-01-04|          AD|                   4.555556|                      1.125|                   8.708333|       3.81|       null| 0.722222|         77.33864|\n",
      "|2020-01-05|          AD|                   4.763889|                       -1.0|                  11.361111|     2.4765|       null|-3.361111|         60.76238|\n",
      "+----------+------------+---------------------------+---------------------------+---------------------------+-----------+-----------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/GoogleCloudPlatform/covid-19-open-data/blob/main/docs/table-weather.md\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('date', DateType(), True),\n",
    "    StructField('location_key', StringType(), True),\n",
    "    StructField('average_temperature_celsius', FloatType(), True),\n",
    "    StructField('minimum_temperature_celsius', FloatType(), True),\n",
    "    StructField('maximum_temperature_celsius', FloatType(), True),\n",
    "    StructField('rainfall_mm', FloatType(), True),\n",
    "    StructField('snowfall_mm', FloatType(), True),\n",
    "    StructField('dew_point', FloatType(), True),\n",
    "    StructField('relative_humidity', FloatType(), True),\n",
    "])\n",
    "\n",
    "weather_df = spark.read.format(\"csv\").schema(schema).option(\"header\", True).load(COVID_WEATHER_FILE)  # \n",
    "\n",
    "weather_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date,location_key,average_temperature_celsius,minimum_temperature_celsius,maximum_temperature_celsius,rainfall_mm,snowfall_mm,dew_point,relative_humidity\n",
      "2020-01-01,AD,4.236111,0.138889,8.208333,3.302,,-0.972222,72.773049\n"
     ]
    }
   ],
   "source": [
    "!head -2 $COVID_WEATHER_FILE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
